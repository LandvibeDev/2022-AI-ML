# 5장
수치 미분은 구현하기는 쉽지만 계산시간이 오래걸린다는 단점이 존재함\
오차역전파 법으로 가중치 매개변수의 기울기를 효율적으로 계산할 수 있음\
오차역전파법을 이해하는 방법으로 두가지 방법이 존재함
1. 수식을 통하여 이해하는 방법
2. 계산 그래프를 통하여 이해하는 방법
----------
## 5.1 계산 그래프
<strong>계산 그래프</strong>는 계산 과정을 그래프로 나타낸 것
- 그래프는 그래프 자료구조와 같이, 복수의 노드와 에지로 표현 됨

![img5-2](https://user-images.githubusercontent.com/58386334/180712813-bef3a16f-19f5-4220-a77c-1cd32e25c152.jpeg)
### 5.1.1 계산 그래프로 풀다
계산 그래프에서 '계산을 왼쪽에서 오른쪽으로 진행'하는 단계를 <strong>순전파</strong>라고함\
순전파의 반대방향인 '오른쪽에서 왼쪽으로'의 전파인 <strong>역전파</strong> 또한 존재함
### 5.1.2 국소적 계산
계산 그래프의 특징으로 '국소적 계산'을 전파함 으로써 최중 결과를 얻는다는 점이 있음\
국소적 계산이란 전체에서 어떤 일이 벌어지든 상관없이 자신과 관계된 정보만으로 결과를 출력할 수 있다는 것임

### 5.1.3 왜 계산 그패르로 푸는가?
계산 그래프를 사용했을 때의 이점이 무엇인가?
- 국소적 계산이 가능하므로, 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화할 수 있음
- 계산 그래프는 중간 계산 결과를 모두 보관할 수 있음
- 역전파를 통해 '미분'을 효율적으로 계산할 수 있음

![img5-5](https://user-images.githubusercontent.com/58386334/180712836-6a6e8a93-9a5c-4efd-9551-ac0897229e55.jpeg)

그래프에서 역전파는 순전파와 반대방향의 화살표(굵은 선)로 그림

------------
## 5.2 연쇄법칙
### 5.2.1 계산 그래프의 역전파
역전파의 절파는 신호 E에 노드의 구소적 미분 ( $dy\over dx$ ) 을 곱한 후 다음 노드로 전달하는 것임
![Img5-6](https://user-images.githubusercontent.com/58386334/180712871-d3eb0c24-8b9f-48a6-aaf9-da407bcfdb3b.jpeg)

이러한 방식을 따르면 목표로 하는 미분 값을 효율적으로 구할 수 있다는 것이 핵심\
이것이 왜 가능한 지는 연쇄법칙의 원리로 설명할 수 있음
### 5.2.2 연쇄법칙이란?
$$
\frac{dz}{dx} = \frac{dz}{dt} \frac{dt}{dx}
$$
### 5.2.3 연쇄법칙과 계산 그래프
![img5-7](https://user-images.githubusercontent.com/58386334/180712910-7023db68-c177-4693-95ab-6bd101ccafed.jpeg)


----------
## 5.3 역전파
### 5.3.1 덧셈 노드의 역전파
덧셈 노드의 역전파는 1을 곱하기만 할 뿐 입력된 값을 그대로 다음 노드로 보냄\
~~$ \frac{dz}{dx} = \frac{dz}{dy} = 1 $ 이기 때문임~~
![img5-9](https://user-images.githubusercontent.com/58386334/180712944-644d8649-ac4c-4d91-9fb1-d47b752d2085.jpeg)

### 5.3.2 곱셈 노드의 역전파
$z = xy$인 경우 $\frac{dz}{dx}=y, \frac{dz}{dy}=x$이므로
![img5-12](https://user-images.githubusercontent.com/58386334/180712957-77fe0d5b-e654-496e-ae6b-a1327fbe1250.jpeg)

와 같이 그래프를 그릴 수 있음\
곰셉 노드 역전파는 상류의 값에 순전파 떄의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보냄
### 5.3.3 사과 쇼핑의 예
사과 쇼핑의 예에서 사과 가격의 미분은 2.2, 사과 개수의 미분은 110, 소비세의 미분은 200으로\
같은 크기의 변화로 최종금액에는 소비세, 사과개수, 사과 가격 순으로 큰 영향을 준다고 해석할 수 있음
![img5-14](https://user-images.githubusercontent.com/58386334/180712968-2fb362f3-05f5-4e0c-8c44-05c2472de067.jpeg)


------------
## 5.5 활성화 함수 계층 구현하기
### 5.5.1 ReLU 계층
활성화 함수 ReLU의 수식은 다음과 같음

$$
y= \begin{cases}
x\;(x>0)\\
0\;(x≤0)
\end{cases}
$$

따라서 $x$에 대한 $y$의 미분은 다음과 같음

$$
\frac{dy}{dx} = \begin{cases}
1\;(x>0)\\
0\;(x≤0)
\end{cases}
$$

ReLU 계층의 계산 그래프는 다음과 같이 그릴 수 있음
![img5-18](https://user-images.githubusercontent.com/58386334/180713038-45a3a657-322c-4fa1-883d-7377d019c0ed.jpeg)

ReLU 계층은 스위치로 비유할 수 있음
### 5.5.2 sigmoid 계층
시그모이드 함수는 다음과 같음

$$
y=\frac{1}{1+exp(-x)}
$$

위 식을 계산 그래프로 그리면 다음과 같음
![img5-19](https://user-images.githubusercontent.com/58386334/180713047-fb1d5a8d-7f43-4d93-8a5f-d243ce59a116.jpeg)

$y=\frac{1}{X}$의 미분은

$$
\begin{aligned}
dy&=-\,\frac{1}{x^2}\\
&=-\;y^2
\end{aligned}
$$

이며, $y=exp(x)$의 미분은

$$
dy=exp(x)
$$

이므로 sigmoid의 역전파를 계산 그래프로 그려보면
![img5-20](https://user-images.githubusercontent.com/58386334/180713088-83e7191e-8132-410b-98e1-198df554d3eb.jpeg)

그런데 $\frac{dL}{dy}y^2exp(-x)$는 다음과 같이 정리된다.

$$
\begin{aligned}
\frac{dL}{dy}y^2exp(-x) &= \frac{dL}{dy}\frac{1}{(1+exp(-x))^2}exp(-x)\\
&=\frac{dL}{dy}\frac{1}{1+exp(-x)}\frac{exp(-x)}{1+exp(-x)}\\
&=\frac{dL}{dy}y(1-y)\\
\end{aligned}
$$

따라서 sigmoid 계층의 역전파는 순전파의 출력만으로 계산이 가능 함
![img5-22](https://user-images.githubusercontent.com/58386334/180713111-c1444fb7-270f-4dce-9768-967527bc3cda.jpeg)

---------
## 5.6 Affine/Softmax 계층 구현하기
### Affine 계층
```
신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서는 어파인 변환(affine translation)이라 함
```
Affine 계층의 계산 그래프는 변수의 형상을 변수명 위에 표기함
![img5-24](https://user-images.githubusercontent.com/58386334/180713138-3f0b70b4-51e3-472e-9c2b-c7d7448928f4.jpeg)

위 계산 그래프를 이용해 역전파에 대해 생각 해보면,\
$\frac{dL}{dX}$와 $X$의 형상이 같아야 하므로\
$\frac{dL}{dX}(2,)$는 $\frac{dL}{dY}(3,)$에 $W^T(3,2)$를 곱해준 값과 같다. \
따라서 형상을 같게 해주기 위해 역전파는 다음과 같다.

$$
\begin{aligned}
\frac{dL}{dX}&=\frac{dL}{dY}W^T\\
\frac{dL}{dW}&=X^T\frac{dL}{dY}\\
\end{aligned}
$$

![img5-25](https://user-images.githubusercontent.com/58386334/180713157-0ad312a9-6951-43d4-bd9b-7003b5109534.jpeg)
### 5.6.3 Softmax-with-Loss 계층
소프트맥스 함수는 출력층에서 입력값을 정규화하여 출력함
- 여기서 정규화란 출력의 합이 1이 되도록 변형 한다는 뜻
- Softmax계층은 학습에서는 필요하지만,
- 추론의 경우는 답을 하나만 내면 되므로 Softmax계층이 필요없음

-----------
## 5.7 오차역전파법 구현하기
### 5.7.1 신경망 학습의 전체 그림
신경망 학습의 순서
- 전제\
신경망에는 적용 가능한 가중치와 편향이 있고, 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 <strong>학습</strong>이라 함\
신경망 학습은 다음 4단계로 수행됨
1. 미니배치\
훈련 데이터 중 일부를 무작위로 가져옴, 이러한 데이터를 미니배치라 하며,\
미니배치의 손실함수 값을 줄이는 것이 목표
2. 기울기 산출\
미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함\
기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시함
3. 매개변수 갱신\
가중치 매개변수를 기울기 방향으로 아주 조금 갱신 함
4. 반복\
1~3단계를 반복 함

